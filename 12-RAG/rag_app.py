import streamlit as st
import tiktoken
import openai
import time
import math

from loguru import logger
from openai.error import RateLimitError

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.retrievers.multi_query import MultiQueryRetriever

from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS


def main():
    st.set_page_config(page_title="MultiQuery RAG", page_icon="ğŸ“š")
    st.title("_MultiQuery ê¸°ë°˜ :red[ë¬¸ì„œ QA]_ ğŸ“š")

    with st.sidebar:
        uploaded_files = st.file_uploader(
            "ğŸ“ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf", "docx", "pptx"], accept_multiple_files=True
        )
        openai_api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")

        process = st.button("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")
    
        st.markdown("### âš™ï¸ ì„ë² ë”© / ì²­í¬ ì„¤ì •")
        max_chunks = st.number_input(
            "ğŸ”¢ ì„ë² ë”©í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (ê°œ)",
            min_value=50,
            max_value=20000,
            value=800,
            step=50,
            help="ì—…ë¡œë“œí•œ ë¬¸ì„œì—ì„œ ìƒì„±ëœ ì²­í¬ ì¤‘ ìƒìœ„ Nê°œë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.",
        )
        chunk_size_ui = st.number_input(
            "ğŸ§© ì²­í¬ í¬ê¸° (í† í° ê·¼ì‚¬)",
            min_value=200,
            max_value=3000,
            value=800,
            step=50,
            help="ì²­í¬ê°€ ì‘ì„ìˆ˜ë¡ ì„ë² ë”© ë‹¹ í† í° ìˆ˜ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.",
        )
        chunk_overlap_ui = st.number_input(
            "ğŸ” ì²­í¬ ì˜¤ë²„ë©",
            min_value=0,
            max_value=1000,
            value=120,
            step=10,
            help="ì¸ì ‘ ì²­í¬ ê°„ ì¤‘ë³µ í† í° ìˆ˜.",
        )

        st.markdown("### ğŸ¢ ë ˆì´íŠ¸ë¦¬ë°‹ ë‚´ì„±")
        batch_size = st.number_input(
            "ğŸ“¦ ì„ë² ë”© ë°°ì¹˜ í¬ê¸°",
            min_value=8,
            max_value=512,
            value=64,
            step=8,
            help="ì´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ OpenAI Embeddings APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.",
        )
        pause_seconds = st.number_input(
            "â±ï¸ ë°°ì¹˜ ê°„ ëŒ€ê¸° (ì´ˆ)",
            min_value=0.0,
            max_value=10.0,
            value=1.5,
            step=0.5,
            help="ë°°ì¹˜ ê°„ ì ê¹ ì‰¬ì–´ OpenAI ë ˆì´íŠ¸ë¦¬ë°‹ì„ í”¼í•©ë‹ˆë‹¤.",
        )


        if st.button("ğŸ” API í‚¤ í…ŒìŠ¤íŠ¸"):
            try:
                # v0.28.x ìŠ¤íƒ€ì¼ (í˜„ì¬ ì½”ë“œì™€ ë™ì¼ ê³„ì—´)
                openai.api_key = openai_api_key
                _ = openai.Embedding.create(
                    model="text-embedding-3-large", input=["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"]
                )
                st.success("âœ… API í‚¤ ì •ìƒì…ë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"âŒ ì‹¤íŒ¨: {e}")

    if process:
        if not openai_api_key:
            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        if not uploaded_files:
            st.warning("í•˜ë‚˜ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        # 1) Load & split documents
        docs = get_text(uploaded_files)
        if not docs:
            st.error("ë¬¸ì„œë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§€ì› í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        chunks = get_text_chunks(
            docs, chunk_size=chunk_size_ui, chunk_overlap=chunk_overlap_ui
        )
        if not chunks:
            st.error("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        if len(chunks) > max_chunks:
            chunks = chunks[: int(max_chunks)]
            st.info(f"ì´ ì²­í¬ê°€ ë§ì•„ ìƒìœ„ {len(chunks)}ê°œë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.")

        # 2) Build vector store with throttled batching
        with st.spinner("ì„ë² ë”© ë° ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."):
            vectorstore = get_vectorstore(chunks, openai_api_key)
        st.success("âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

        # 3) Build retriever chain
        chain = get_multiquery_chain(vectorstore, openai_api_key)
        st.session_state.conversation = chain
        st.session_state.chat_history = []

    # Chat UI
    if "conversation" in st.session_state:
        for msg in st.session_state.chat_history:
            with st.chat_message("user"):
                st.markdown(msg["question"])
            with st.chat_message("assistant"):
                st.markdown(msg["answer"])

        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            with st.chat_message("user"):
                st.markdown(query)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation.invoke(
                            {"question": query}
                        )
                    except Exception as e:
                        response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                    st.markdown(response)

                st.session_state.chat_history.append(
                    {"question": query, "answer": response}
                )




def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))


def get_text(docs):
    doc_list = []
    for doc in docs:
        file_name = doc.name
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())
        logger.info(f"Uploaded {file_name}")

        if '.pdf' in doc.name:
            loader = PyMuPDFLoader(file_name)
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
        else:
            continue

        doc_list.extend(loader.load_and_split())
    return doc_list


def get_text_chunks(docs, chunk_size=800, chunk_overlap=120):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=tiktoken_len,
    )
    return [doc for doc in splitter.split_documents(docs) if doc.page_content.strip()]


def get_vectorstore(text_chunks, api_key):
    """
    Build FAISS index in small batches with live ETA.
    - Shows a Streamlit progress bar with remaining time.
    - Handles OpenAI rate limits via backoff.
    """
    # You can tune these without changing any other code:
    BATCH_SIZE = 64
    PAUSE_BETWEEN_BATCHES = 1.5  # seconds

    embeddings = OpenAIEmbeddings(
        model_name="text-embedding-3-small",
        openai_api_key=api_key
    )

    texts = [doc.page_content for doc in text_chunks]
    metadatas = [doc.metadata for doc in text_chunks]
    total = len(texts)
    if total == 0:
        return FAISS.from_texts(texts=[], embedding=embeddings, metadatas=[])

    # Helpers
    def _fmt_eta(sec: float) -> str:
        sec = max(0, int(sec))
        m, s = divmod(sec, 60)
        h, m = divmod(m, 60)
        return f"{h:d}:{m:02d}:{s:02d}" if h else f"{m:d}:{s:02d}"

    progress = st.progress(0.0, text=f"ì„ë² ë”©/ì¸ë±ì‹± ì‹œì‘... (0/{total})")
    start_t = time.perf_counter()

    vs = None
    i = 0
    ema_per_item = None        # exponential moving average of seconds per item
    alpha = 0.25               # smoothing factor

    while i < total:
        j = min(i + BATCH_SIZE, total)
        batch_texts = texts[i:j]
        batch_metas = metadatas[i:j]

        batch_start = time.perf_counter()
        backoff = 2.0

        while True:
            try:
                if vs is None:
                    vs = FAISS.from_texts(
                        texts=batch_texts,
                        embedding=embeddings,
                        metadatas=batch_metas
                    )
                else:
                    vs.add_texts(texts=batch_texts, metadatas=batch_metas)
                break
            except RateLimitError:
                time.sleep(backoff)
                backoff = min(30.0, backoff * 1.8)

        # Optional gentle pacing between batches
        if j < total:
            time.sleep(PAUSE_BETWEEN_BATCHES)

        # Update timing stats
        batch_time = time.perf_counter() - batch_start
        items = j - i
        per_item = batch_time / max(1, items)
        ema_per_item = per_item if ema_per_item is None else (
            alpha * per_item + (1 - alpha) * ema_per_item
        )

        done = j
        remaining = total - done
        # Add expected sleep cost for remaining batches
        remaining_batches = math.ceil(remaining / BATCH_SIZE) if BATCH_SIZE else 0
        eta_sec = (ema_per_item * remaining) + (PAUSE_BETWEEN_BATCHES * remaining_batches)

        frac = done / total
        progress.progress(
            frac,
            text=f"ì„ë² ë”©/ì¸ë±ì‹± ì§„í–‰ ì¤‘... {done}/{total} Â· ETA {_fmt_eta(eta_sec)}"
        )

        i = j

    total_time = time.perf_counter() - start_t
    progress.progress(1.0, text=f"ì„ë² ë”©/ì¸ë±ì‹± ì™„ë£Œ! ì´ ì†Œìš” {_fmt_eta(total_time)}")
    return vs

    texts = [doc.page_content for doc in text_chunks]
    metadatas = [doc.metadata for doc in text_chunks]
    return FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)


def get_multiquery_chain(vectorstore, api_key):
    llm = ChatOpenAI(
        model_name="gpt-5",
        openai_api_key=api_key,
        temperature=0
    )

    retriever = MultiQueryRetriever.from_llm(
        retriever=vectorstore.as_retriever(), llm=llm
    )

    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì±…ì˜ ë‚´ìš©(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ë‹¤ìŒ ì±…ì˜ ë‚´ìš©(context) ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë¬¸ì„œì— ì§ì ‘ì ì¸ ì„¤ëª…ì´ì—†ë”ë¼ë„, ë¬¸ë§¥ì„ í†µí•´ ìœ ì¶”ë¥¼ í•´ë³´ê³  ìƒê°ì„ ê³ë“¤ì—¬ë„ ê´œì°®ì•„. ê·¸ë¦¬ê³ ë„ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

    ê·¸ë¦¬ê³ , ë‹¹ì‹ ì€ ë‹¹ì‹ ì˜ ì§€ëŠ¥ì— ëŒ€í•œ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì£¼ê¸°ìœ„í•´, ë¬¸ë§¥ì— ì˜ì§€í•˜ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ”ê±´ í”¼í•´ì•¼í•©ë‹ˆë‹¤. 'ë¬¸ë§¥ìƒ...'ìœ¼ë¡œ ë¬¸ì¥ì„ ì‹œì‘í•˜ë©´ ë‹¹ì‹ ì´ ë‹¹ì‹ ì˜ ëŒ€ë‹µì— ëŒ€í•´ ì±…ì„ì„ íšŒí”¼í•˜ë ¤ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ë¯€ë¡œ ì´ë¥¼ í”¼í•´ì•¼í•©ë‹ˆë‹¤.

    #Context:
    {context}

    #Question:
    {question}

    #Answer:"""
    )

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain


if __name__ == "__main__":
    main()

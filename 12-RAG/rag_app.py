import streamlit as st
import tiktoken
from loguru import logger
import openai

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.retrievers.multi_query import MultiQueryRetriever

from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS


def main():
    st.set_page_config(page_title="MultiQuery RAG", page_icon="ğŸ“š")
    st.title("_MultiQuery ê¸°ë°˜ :red[ë¬¸ì„œ QA]_ ğŸ“š")

    with st.sidebar:
        uploaded_files = st.file_uploader("ğŸ“ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf', 'docx'], accept_multiple_files=True)
        openai_api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")
        process = st.button("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")

        if st.button("ğŸ” API í‚¤ í…ŒìŠ¤íŠ¸"):
            try:
                openai.api_key = openai_api_key
                resp = openai.Embedding.create(
                    model="text-embedding-3-small",
                    input=["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"]
                )
                st.success("âœ… API í‚¤ ì •ìƒì…ë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"âŒ ì‹¤íŒ¨: {e}")

    if process:
        if not openai_api_key:
            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        docs = get_text(uploaded_files)
        chunks = get_text_chunks(docs)
        vectorstore = get_vectorstore(chunks, openai_api_key)
        chain = get_multiquery_chain(vectorstore, openai_api_key)
        st.session_state.conversation = chain
        st.session_state.chat_history = []

    if "conversation" in st.session_state:
        for msg in st.session_state.chat_history:
            with st.chat_message("user"):
                st.markdown(msg["question"])
            with st.chat_message("assistant"):
                st.markdown(msg["answer"])

        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            with st.chat_message("user"):
                st.markdown(query)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    response = st.session_state.conversation.invoke({"question": query})
                    st.markdown(response)

                st.session_state.chat_history.append({
                    "question": query,
                    "answer": response
                })


def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))


def get_text(docs):
    doc_list = []
    for doc in docs:
        file_name = doc.name
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())
        logger.info(f"Uploaded {file_name}")

        if '.pdf' in doc.name:
            loader = PyMuPDFLoader(file_name)
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
        else:
            continue

        doc_list.extend(loader.load_and_split())
    return doc_list


def get_text_chunks(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=tiktoken_len
    )
    return [doc for doc in splitter.split_documents(docs) if doc.page_content.strip()]


def get_vectorstore(text_chunks, api_key):
    embeddings = OpenAIEmbeddings(
        model_name="text-embedding-3-small",
        openai_api_key=api_key
    )
    texts = [doc.page_content for doc in text_chunks]
    metadatas = [doc.metadata for doc in text_chunks]
    return FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)


def get_multiquery_chain(vectorstore, api_key):
    llm = ChatOpenAI(
        model_name="gpt-4.1-2025-04-14",
        openai_api_key=api_key,
        temperature=0
    )

    retriever = MultiQueryRetriever.from_llm(
        retriever=vectorstore.as_retriever(), llm=llm
    )

    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì±…ì˜ ë‚´ìš©(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ë‹¤ìŒ ì±…ì˜ ë‚´ìš©(context) ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë¬¸ì„œì— ì§ì ‘ì ì¸ ì„¤ëª…ì´ì—†ë”ë¼ë„, ë¬¸ë§¥ì„ í†µí•´ ìœ ì¶”ë¥¼ í•´ë³´ê³  ìƒê°ì„ ê³ë“¤ì—¬ë„ ê´œì°®ì•„. ê·¸ë¦¬ê³ ë„ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

    ê·¸ë¦¬ê³ , ë‹¹ì‹ ì€ ë‹¹ì‹ ì˜ ì§€ëŠ¥ì— ëŒ€í•œ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì£¼ê¸°ìœ„í•´, ë¬¸ë§¥ì— ì˜ì§€í•˜ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ”ê±´ í”¼í•´ì•¼í•©ë‹ˆë‹¤. 'ë¬¸ë§¥ìƒ...'ìœ¼ë¡œ ë¬¸ì¥ì„ ì‹œì‘í•˜ë©´ ë‹¹ì‹ ì´ ë‹¹ì‹ ì˜ ëŒ€ë‹µì— ëŒ€í•´ ì±…ì„ì„ íšŒí”¼í•˜ë ¤ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ë¯€ë¡œ ì´ë¥¼ í”¼í•´ì•¼í•©ë‹ˆë‹¤.

    #Context:
    {context}

    #Question:
    {question}

    #Answer:"""
    )

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain


if __name__ == "__main__":
    main()

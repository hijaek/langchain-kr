import streamlit as st
import tiktoken
import openai
import time
import math

from loguru import logger
from openai.error import RateLimitError

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.retrievers.multi_query import MultiQueryRetriever

from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS


def main():
    st.set_page_config(page_title="MultiQuery RAG", page_icon="ğŸ“š")
    st.title("_MultiQuery ê¸°ë°˜ :red[ë¬¸ì„œ QA]_ ğŸ“š")

    with st.sidebar:
        uploaded_files = st.file_uploader(
            "ğŸ“ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf", "docx", "pptx"], accept_multiple_files=True
        )
        openai_api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password")

        process = st.button("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")
    
        st.markdown("### âš™ï¸ ì„ë² ë”© / ì²­í¬ ì„¤ì •")
        max_chunks = st.number_input(
            "ğŸ”¢ ì„ë² ë”©í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (ê°œ)",
            min_value=50,
            max_value=20000,
            value=800,
            step=50,
            help="ì—…ë¡œë“œí•œ ë¬¸ì„œì—ì„œ ìƒì„±ëœ ì²­í¬ ì¤‘ ìƒìœ„ Nê°œë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.",
        )
        chunk_size_ui = st.number_input(
            "ğŸ§© ì²­í¬ í¬ê¸° (í† í° ê·¼ì‚¬)",
            min_value=200,
            max_value=3000,
            value=800,
            step=50,
            help="ì²­í¬ê°€ ì‘ì„ìˆ˜ë¡ ì„ë² ë”© ë‹¹ í† í° ìˆ˜ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.",
        )
        chunk_overlap_ui = st.number_input(
            "ğŸ” ì²­í¬ ì˜¤ë²„ë©",
            min_value=0,
            max_value=1000,
            value=120,
            step=10,
            help="ì¸ì ‘ ì²­í¬ ê°„ ì¤‘ë³µ í† í° ìˆ˜.",
        )

        st.markdown("### ğŸ¢ ë ˆì´íŠ¸ë¦¬ë°‹ ë‚´ì„±")
        batch_size = st.number_input(
            "ğŸ“¦ ì„ë² ë”© ë°°ì¹˜ í¬ê¸°",
            min_value=8,
            max_value=512,
            value=64,
            step=8,
            help="ì´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ OpenAI Embeddings APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.",
        )
        pause_seconds = st.number_input(
            "â±ï¸ ë°°ì¹˜ ê°„ ëŒ€ê¸° (ì´ˆ)",
            min_value=0.0,
            max_value=10.0,
            value=1.5,
            step=0.5,
            help="ë°°ì¹˜ ê°„ ì ê¹ ì‰¬ì–´ OpenAI ë ˆì´íŠ¸ë¦¬ë°‹ì„ í”¼í•©ë‹ˆë‹¤.",
        )


        if st.button("ğŸ” API í‚¤ í…ŒìŠ¤íŠ¸"):
            try:
                # v0.28.x ìŠ¤íƒ€ì¼ (í˜„ì¬ ì½”ë“œì™€ ë™ì¼ ê³„ì—´)
                openai.api_key = openai_api_key
                _ = openai.Embedding.create(
                    model="text-embedding-3-large", input=["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"]
                )
                st.success("âœ… API í‚¤ ì •ìƒì…ë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"âŒ ì‹¤íŒ¨: {e}")

    if process:
        if not openai_api_key:
            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        if not uploaded_files:
            st.warning("í•˜ë‚˜ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        # 1) Load & split documents
        docs = get_text(uploaded_files)
        if not docs:
            st.error("ë¬¸ì„œë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§€ì› í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        chunks = get_text_chunks(
            docs, chunk_size=chunk_size_ui, chunk_overlap=chunk_overlap_ui
        )
        if not chunks:
            st.error("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        if len(chunks) > max_chunks:
            chunks = chunks[: int(max_chunks)]
            st.info(f"ì´ ì²­í¬ê°€ ë§ì•„ ìƒìœ„ {len(chunks)}ê°œë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.")

        # 2) Build vector store with throttled batching
        with st.spinner("ì„ë² ë”© ë° ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."):
            vectorstore = get_vectorstore(chunks, openai_api_key)
        st.success("âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

        # 3) Build retriever chain
        chain = get_multiquery_chain(vectorstore, openai_api_key)
        st.session_state.conversation = chain
        st.session_state.chat_history = []

    # Chat UI
    if "conversation" in st.session_state:
        for msg in st.session_state.chat_history:
            with st.chat_message("user"):
                st.markdown(msg["question"])
            with st.chat_message("assistant"):
                st.markdown(msg["answer"])

        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            with st.chat_message("user"):
                st.markdown(query)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        response = st.session_state.conversation.invoke(
                            {"question": query}
                        )
                    except Exception as e:
                        response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                    st.markdown(response)

                st.session_state.chat_history.append(
                    {"question": query, "answer": response}
                )




def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))


def get_text(docs):
    doc_list = []
    for doc in docs:
        file_name = doc.name
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())
        logger.info(f"Uploaded {file_name}")

        if '.pdf' in doc.name:
            loader = PyMuPDFLoader(file_name)
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
        else:
            continue

        doc_list.extend(loader.load_and_split())
    return doc_list


def get_text_chunks(docs, chunk_size=800, chunk_overlap=120):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=tiktoken_len,
    )
    return [doc for doc in splitter.split_documents(docs) if doc.page_content.strip()]


def get_vectorstore(text_chunks, api_key, batch_size=32, pause=1.5,
                    max_retries_per_batch=8, max_backoff=30.0):
    """
    - ë°°ì¹˜/ë°±ì˜¤í”„ë¡œ ì„ë² ë”© + ì§„í–‰ë¥ /ETA í‘œì‹œ
    - ë¹„íšŒë³µì„± ì˜¤ë¥˜(ì˜ˆ: insufficient_quota)ëŠ” ì¦‰ì‹œ í‘œì‹œí•˜ê³  ì¤‘ë‹¨
    - ì²« ë°°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì‘ê²Œ ë³´ë‚´ì„œ 0/x ì •ì²´ ë°©ì§€
    """
    # OpenAIEmbeddingsì˜ ìš”ì²­ë‹¹ ë¬¶ìŒ í¬ê¸°(ë‚´ë¶€ ë§ˆì´í¬ë¡œë°°ì¹˜)ë„ ì‘ê²Œ
    try:
        embeddings = OpenAIEmbeddings(
            model_name="text-embedding-3-small",
            openai_api_key=api_key,
            chunk_size=max(1, min(8, batch_size)),  # ë„ˆë¬´ í¬ê²Œ ë¬¶ì§€ ì•Šê¸°
            max_retries=0,  # ìš°ë¦¬ ìª½ì—ì„œ ì¬ì‹œë„ ì œì–´
            request_timeout=60,
        )
    except TypeError:
        # ì¼ë¶€ ë²„ì „ í˜¸í™˜
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=api_key,
            chunk_size=max(1, min(8, batch_size)),
            max_retries=0,
            request_timeout=60,
        )

    texts = [d.page_content for d in text_chunks]
    metas = [d.metadata for d in text_chunks]
    total = len(texts)
    if total == 0:
        return FAISS.from_texts(texts=[], embedding=embeddings, metadatas=[])

    # ETA í‘œì‹œ ìœ í‹¸
    def fmt_eta(sec: float) -> str:
        sec = max(0, int(sec))
        m, s = divmod(sec, 60); h, m = divmod(m, 60)
        return f"{h:d}:{m:02d}:{s:02d}" if h else f"{m:d}:{s:02d}"

    progress = st.progress(0.0, text=f"ì„ë² ë”©/ì¸ë±ì‹± ì‹œì‘... (0/{total})")
    status = st.empty()
    start_t = time.perf_counter()
    ema_per_item = None
    alpha = 0.25

    # ì²« ë°°ì¹˜ëŠ” ë°˜ë“œì‹œ ì•„ì£¼ ì‘ê²Œ(ì˜ˆ: 1~4ê°œ) -> 0ì—ì„œ ë©ˆì¶¤ ë°©ì§€
    tiny_first_batch = max(1, min(4, batch_size))

    vs = None
    i = 0
    while i < total:
        # ì²« ë£¨í”„ë§Œ ultra small, ì´í›„ë¶€í„°ëŠ” UI batch_size ì‚¬ìš©
        step = tiny_first_batch if i == 0 else batch_size
        j = min(i + step, total)
        tbatch = texts[i:j]
        mbatch = metas[i:j]

        attempt = 0
        backoff = 2.0
        batch_start = time.perf_counter()

        while True:
            try:
                if vs is None:
                    vs = FAISS.from_texts(texts=tbatch, embedding=embeddings, metadatas=mbatch)
                else:
                    vs.add_texts(texts=tbatch, metadatas=mbatch)
                break  # ì„±ê³µ
            except RateLimitError as e:
                msg = str(e).lower()
                # ë¹„íšŒë³µì„±(í¬ë ˆë”§/ì¿¼í„°)ì¼ ê°€ëŠ¥ì„±
                if "insufficient_quota" in msg or "exceeded your current quota" in msg:
                    status.error("âŒ OpenAI ì”ì•¡/ì¿¼í„° ë¶€ì¡±ìœ¼ë¡œ ì„ë² ë”©ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    raise
                attempt += 1
                if attempt > max_retries_per_batch:
                    status.error("âŒ ë ˆì´íŠ¸ë¦¬ë°‹ ì¬ì‹œë„ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”.")
                    raise
                wait_s = min(max_backoff, backoff)
                status.write(f"â³ ë ˆì´íŠ¸ë¦¬ë°‹ ëŒ€ê¸° ì¤‘â€¦ {wait_s:.1f}s (ì¬ì‹œë„ {attempt}/{max_retries_per_batch})")
                time.sleep(wait_s)
                backoff *= 1.8
            except Exception as e:
                status.error(f"âŒ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜: {e}")
                raise

        # ë°°ì¹˜ ì‚¬ì´ ì†Œí­ ëŒ€ê¸° (ê³¼ë„í•œ ì—°ì† í˜¸ì¶œ ë°©ì§€)
        if j < total and pause > 0:
            time.sleep(pause)

        # ì§„í–‰ë¥ /ETA ê°±ì‹ 
        batch_time = time.perf_counter() - batch_start
        items = j - i
        per_item = batch_time / max(1, items)
        ema_per_item = per_item if ema_per_item is None else (alpha * per_item + (1 - alpha) * ema_per_item)

        done = j
        remaining = total - done
        remaining_batches = math.ceil(remaining / max(1, batch_size))
        eta_sec = (ema_per_item * remaining) + (pause * remaining_batches)

        progress.progress(done / total, text=f"ì„ë² ë”©/ì¸ë±ì‹± ì§„í–‰ ì¤‘... {done}/{total} Â· ETA {fmt_eta(eta_sec)}")
        i = j

    total_time = time.perf_counter() - start_t
    status.empty()
    progress.progress(1.0, text=f"ì„ë² ë”©/ì¸ë±ì‹± ì™„ë£Œ! ì´ ì†Œìš” {fmt_eta(total_time)}")
    return vs


    texts = [doc.page_content for doc in text_chunks]
    metadatas = [doc.metadata for doc in text_chunks]
    return FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)

vectorstore = get_vectorstore(
    chunks, openai_api_key, batch_size=batch_size, pause=pause_seconds
)


def get_multiquery_chain(vectorstore, api_key):
    llm = ChatOpenAI(
        model_name="gpt-5",
        openai_api_key=api_key,
        temperature=0
    )

    retriever = MultiQueryRetriever.from_llm(
        retriever=vectorstore.as_retriever(), llm=llm
    )

    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì±…ì˜ ë‚´ìš©(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ë‹¤ìŒ ì±…ì˜ ë‚´ìš©(context) ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë¬¸ì„œì— ì§ì ‘ì ì¸ ì„¤ëª…ì´ì—†ë”ë¼ë„, ë¬¸ë§¥ì„ í†µí•´ ìœ ì¶”ë¥¼ í•´ë³´ê³  ìƒê°ì„ ê³ë“¤ì—¬ë„ ê´œì°®ì•„. ê·¸ë¦¬ê³ ë„ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
    í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

    ê·¸ë¦¬ê³ , ë‹¹ì‹ ì€ ë‹¹ì‹ ì˜ ì§€ëŠ¥ì— ëŒ€í•œ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì£¼ê¸°ìœ„í•´, ë¬¸ë§¥ì— ì˜ì§€í•˜ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ”ê±´ í”¼í•´ì•¼í•©ë‹ˆë‹¤. 'ë¬¸ë§¥ìƒ...'ìœ¼ë¡œ ë¬¸ì¥ì„ ì‹œì‘í•˜ë©´ ë‹¹ì‹ ì´ ë‹¹ì‹ ì˜ ëŒ€ë‹µì— ëŒ€í•´ ì±…ì„ì„ íšŒí”¼í•˜ë ¤ëŠ”ê²ƒì²˜ëŸ¼ ë³´ì´ë¯€ë¡œ ì´ë¥¼ í”¼í•´ì•¼í•©ë‹ˆë‹¤.

    #Context:
    {context}

    #Question:
    {question}

    #Answer:"""
    )

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain


if __name__ == "__main__":
    main()
